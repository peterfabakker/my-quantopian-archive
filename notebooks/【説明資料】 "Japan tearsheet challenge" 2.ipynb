{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan challenge submission template\n",
    "\n",
    "+ ライブラリとドメインを import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.pipeline.data import EquityPricing, factset\n",
    "from quantopian.pipeline.factors import Returns, SimpleMovingAverage\n",
    "from quantopian.pipeline.domain import (\n",
    "    AT_EQUITIES, # Austria\n",
    "    AU_EQUITIES, # Australia\n",
    "    BE_EQUITIES, # Belgium\n",
    "    BR_EQUITIES, # Brazil\n",
    "    CA_EQUITIES, # Canada\n",
    "    CH_EQUITIES, # Switzerland\n",
    "    CN_EQUITIES, # China\n",
    "    DE_EQUITIES, # Germany\n",
    "    DK_EQUITIES, # Denmark\n",
    "    ES_EQUITIES, # Spain\n",
    "    FI_EQUITIES, # Finland\n",
    "    FR_EQUITIES, # France\n",
    "    GB_EQUITIES, # Great Britain\n",
    "    HK_EQUITIES, # Hong Kong\n",
    "    IE_EQUITIES, # Ireland\n",
    "    IN_EQUITIES, # India\n",
    "    IT_EQUITIES, # Italy\n",
    "    JP_EQUITIES, # Japan\n",
    "    KR_EQUITIES, # South Korea\n",
    "    NL_EQUITIES, # Netherlands\n",
    "    NO_EQUITIES, # Norway\n",
    "    NZ_EQUITIES, # New Zealand\n",
    "    PT_EQUITIES, # Portugal\n",
    "    SE_EQUITIES, # Sweden\n",
    "    SG_EQUITIES, # Singapore\n",
    "    US_EQUITIES, # United States\n",
    ")\n",
    "from quantopian.research import run_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions\n",
    "\n",
    "ヘルパー関数。ここは一歳触る必要なし。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_factor(factor, \n",
    "                    domain, \n",
    "                    start_date, \n",
    "                    end_date,\n",
    "                    factor_screen=None,\n",
    "                    quantiles=5,\n",
    "                    returns_lengths=(1, 5, 10)):\n",
    "    \"\"\"Analyze a Pipeline Factor using Alphalens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor : quantopian.pipeline.factors.Factor\n",
    "        Factor producing scores to be evaluated.\n",
    "    domain : quantopian.pipeline.domain.Domain\n",
    "        Domain on which the factor should be evaluated.\n",
    "    start_date : str or pd.Timestamp\n",
    "        Start date for evaluation period.\n",
    "    end_date : str or pd.Timestamp\n",
    "        End date for evaluation period.\n",
    "    standardize : \n",
    "    factor_screen : quantopian.pipeline.filters.Filter, optional\n",
    "        Filter defining which assets ``factor`` should be evaluated on.\n",
    "        Default is ``factor.notnull()``.\n",
    "    quantiles : int, optional\n",
    "        Number of buckets to use for quantile groups. Default is 5\n",
    "    returns_lengths : sequence[int]\n",
    "        Forward-returns horizons to use when evaluating ``factor``. \n",
    "        Default is 1-day, 5-day, and 10-day returns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    factor_data : pd.DataFrame\n",
    "        A (date, asset)-indexed DataFrame with the following columns:\n",
    "            'factor': float64\n",
    "                Values produced by ``factor``.\n",
    "            'factor_quantiles': int64\n",
    "                Daily quantile label for each\n",
    "    \"\"\"\n",
    "    calendar = domain.calendar\n",
    "    # Roll input dates to the next trading session.\n",
    "    # 入力された日付の「次の」トレーディングセッション日.\n",
    "    # マーケットオープン前であればその日、クローズ後であれば次の日\n",
    "    start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "    end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "    if factor_screen is None:\n",
    "        factor_screen = factor.notnull()\n",
    "        \n",
    "    # Run pipeline to get factor values and quantiles.\n",
    "    # 【Factor パイプライン作成】\n",
    "    # ここで、Factor計算と、その結果をquantilesに分けてどのQuantileに入っているかを\n",
    "    factor_pipe = Pipeline(\n",
    "        # 各銘柄のファクター \n",
    "        {'factor': factor, \n",
    "         #そのファクターがどのQuantileに入っているか。つまり、0,1,2,3,4 のいずれか\n",
    "         'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "        screen=factor_screen,\n",
    "        domain=domain,\n",
    "    )\n",
    "    \n",
    "    # 【Factor パイプライン実行】    \n",
    "    factor_results = run_pipeline(factor_pipe, start_date, end_date, chunksize=250)\n",
    "    \n",
    "    # 【結果パイプラインを作成】\n",
    "    # Universeに入っている銘柄の毎日のReturnを計算する。\n",
    "    column_order = []\n",
    "    returns_cols = {}\n",
    "    \n",
    "    # まずは、returns_lengths（1日、5日、10日）のコラムを持つDataFrameを作り\n",
    "    # そこにいったんDayReturnの結果を入れる。箱だけ作っている状態。\n",
    "    for length in returns_lengths:\n",
    "        colname = '{}D'.format(length)\n",
    "        column_order.append(colname)\n",
    "        # Here we are not computing cumulative returns, this could be done\n",
    "        # more efficiently\n",
    "        returns_cols[colname] = Returns(window_length=2)\n",
    "    # Universe を渡して、Pipelineを作る。\n",
    "    returns_pipe = Pipeline(returns_cols, domain=domain)\n",
    "    \n",
    "    \n",
    "    # Compute returns for the period after the factor pipeline, then \n",
    "    # shift the results back to align with our factor values.\n",
    "    # 【結果パイプライン実行】    \n",
    "    returns_start_date = start_date\n",
    "    returns_end_date = end_date + domain.calendar.day * max(returns_lengths)\n",
    "    raw_returns = run_pipeline(returns_pipe, returns_start_date, returns_end_date, chunksize=500)\n",
    "\n",
    "    # 【Factorパイプライン結果と、結果パイプライン結果をマージ】\n",
    "    # returns_lengthsに入っている日付で shift しながらデータをマージ\n",
    "    # DataFrameを作る\n",
    "    shifted_returns = {}\n",
    "    for name, length in zip(column_order, returns_lengths):\n",
    "        # Shift 1-day returns back by a day, 5-day returns back by 5 days, etc.\n",
    "        raw = raw_returns[name]\n",
    "        shifted_returns[name] = backshift_returns_series(raw, length)\n",
    "        \n",
    "    # Merge backshifted returns into a single frame indexed like our desired output.\n",
    "    merged_returns = pd.DataFrame(\n",
    "        data=shifted_returns, \n",
    "        index=factor_results.index, \n",
    "        columns=column_order,\n",
    "    )\n",
    "    \n",
    "    # Concat factor results and forward returns column-wise.\n",
    "    merged = pd.concat([factor_results, merged_returns], axis=1)\n",
    "    merged.index.set_names(['date', 'asset'], inplace=True)\n",
    "    \n",
    "    # Drop NaNs\n",
    "    merged = merged.dropna(how='any')\n",
    "    \n",
    "    # Add a Business Day Offset to the DateTimeIndex\n",
    "    merged.index.levels[0].freq = pd.tseries.offsets.BDay()\n",
    "    \n",
    "    # merged は、date と asset をindexに、 コラムに factor / factor quantile / 1日〜１５日の shiftしたDay Return\n",
    "    # print merged.head()\n",
    "    # date                      asset                                       \n",
    "    # 2015-06-01 00:00:00+00:00 Equity(1178883450164305 [4118]) -0.194696   \n",
    "    #                           Equity(1178883465819716 [7458]) -0.078316   \n",
    "    #                           Equity(1178883518248535 [9438])  1.049684   \n",
    "    #                           Equity(1178883767611984 [1950])  0.440113   \n",
    "    #                           Equity(1178883868471374 [8715])  1.565080   \n",
    "\n",
    "    #                                                            factor_quantile  \\\n",
    "    # date                      asset                                              \n",
    "    # 2015-06-01 00:00:00+00:00 Equity(1178883450164305 [4118])                1   \n",
    "    #                           Equity(1178883465819716 [7458])                2   \n",
    "    #                           Equity(1178883518248535 [9438])                4   \n",
    "    #                           Equity(1178883767611984 [1950])                3   \n",
    "    #                           Equity(1178883868471374 [8715])                4   \n",
    "\n",
    "    #                                                                  1D        2D  \\\n",
    "    # date                      asset                                                 \n",
    "    # 2015-06-01 00:00:00+00:00 Equity(1178883450164305 [4118])  0.014301 -0.004338   \n",
    "    #                           Equity(1178883465819716 [7458])  0.012270  0.001212   \n",
    "    #                           Equity(1178883518248535 [9438])  0.021357  0.003690   \n",
    "    #                           Equity(1178883767611984 [1950]) -0.014299 -0.003744   \n",
    "    #                           Equity(1178883868471374 [8715])  0.017296  0.017450   \n",
    "    \n",
    "    return merged\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "    assert new_date_labels[0] == 0\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "    return pd.Series(data=new_values, index=new_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を算出する関数群。これも触る必要はない。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "\n",
    "def compute_turnover(df):\n",
    "    # df は、\n",
    "    # index に、 date と asset \n",
    "    # columns に factor(s) で算出された各銘柄の値を持つDataFrame\n",
    "    # 例：\n",
    "    #     date                       asset                          \n",
    "    # 2015-06-01 00:00:00+00:00  Equity(1178883450164305 [4118])   -0.000231\n",
    "    #                            Equity(1178883465819716 [7458])   -0.000093\n",
    "    #                            Equity(1178883518248535 [9438])    0.001245\n",
    "    #                            Equity(1178883767611984 [1950])    0.000522\n",
    "    #                            Equity(1178883868471374 [8715])    0.001857\n",
    "    # \n",
    "    \n",
    "    # よって、この返り値は、欠損値を削除した後、unstackして date を index \n",
    "    # asset をコラム名にした DataFrameを作り\n",
    "    # 各銘柄の毎日の diff (前日からの引き算)を取得して、絶対値をとり、\n",
    "    # 足し算している。 sum(level=1) なので row の計算。つまり、毎日全銘柄の結果を足し合わせて\n",
    "    # date 毎に出している。\n",
    "    \n",
    "    #     date\n",
    "    #     2015-06-01 00:00:00+00:00    0.000000\n",
    "    #     2015-06-02 00:00:00+00:00    0.076373\n",
    "    #     2015-06-03 00:00:00+00:00    0.032417\n",
    "    #     2015-06-04 00:00:00+00:00    0.029175\n",
    "    #     2015-06-05 00:00:00+00:00    0.045035\n",
    "    #     Freq: B, dtype: float64\n",
    "        \n",
    "    return df.dropna().unstack().dropna(how='all').fillna(0).diff().abs().sum(1)\n",
    "\n",
    "def get_max_median_position_concentration(expos):\n",
    "    # expos は毎日のFactorの計算結果\n",
    "    # date                       asset                          \n",
    "    # 2015-06-01 00:00:00+00:00  Equity(1178883450164305 [4118])   -0.000231\n",
    "    #                            Equity(1178883465819716 [7458])   -0.000093\n",
    "    #                            Equity(1178883518248535 [9438])    0.001245\n",
    "    #                            Equity(1178883767611984 [1950])    0.000522\n",
    "    #                            Equity(1178883868471374 [8715])    0.001857    \n",
    "    longs = expos.loc[expos > 0]\n",
    "    shorts = expos.loc[expos < 0]\n",
    "\n",
    "    # 返り値は index に date, コラムに factor結果の quantile を持つ dataframe \n",
    "    #                                0.05      0.25      0.50      0.75      0.95\n",
    "    # date                                                                       \n",
    "    # 2015-06-01 00:00:00+00:00 -0.002378 -0.000809  0.000150  0.000846  0.001991\n",
    "    # 2015-06-02 00:00:00+00:00 -0.002366 -0.000812  0.000138  0.000830  0.002056\n",
    "    # 2015-06-03 00:00:00+00:00 -0.002368 -0.000814  0.000138  0.000832  0.002063\n",
    "    # 2015-06-04 00:00:00+00:00 -0.002372 -0.000814  0.000141  0.000833  0.002071\n",
    "    # 2015-06-05 00:00:00+00:00 -0.002364 -0.000816  0.000131  0.000821  0.002120\n",
    "            \n",
    "    return expos.groupby(level=0).quantile([.05, .25, .5, .75, .95]).unstack()\n",
    "\n",
    "def compute_factor_stats(factor_data_total, periods=range(1, 15)):\n",
    "    portfolio_returns_total = al.performance.factor_returns(factor_data_total)\n",
    "    # http://quantopian.github.io/alphalens/alphalens.html?highlight=performance#alphalens.performance.factor_returns\n",
    "    # print portfolio_returns_total.head()\n",
    "    #                                  1D        2D        3D        4D        5D  \\\n",
    "    # date                                                                          \n",
    "    # 2015-06-01 00:00:00+00:00  0.002649  0.000449  0.003411  0.000520  0.000578   \n",
    "    # 2015-06-02 00:00:00+00:00  0.000442  0.002991  0.000836  0.000215  0.001074   \n",
    "    # 2015-06-03 00:00:00+00:00  0.002945  0.001080  0.000713  0.000693 -0.000618   \n",
    "    # 2015-06-04 00:00:00+00:00  0.001002  0.000604  0.000880 -0.000612  0.002325   \n",
    "    # 2015-06-05 00:00:00+00:00  0.000831  0.000502 -0.000662  0.001882  0.000483   \n",
    "\n",
    "    #                                  6D        7D        8D        9D       10D  \\\n",
    "    # date                                                                          \n",
    "    # 2015-06-01 00:00:00+00:00  0.001675  0.000180  0.001827  0.000246  0.002402   \n",
    "    # 2015-06-02 00:00:00+00:00 -0.000442  0.002082  0.000783  0.002465  0.000328   \n",
    "    # 2015-06-03 00:00:00+00:00  0.002147  0.000660  0.002547  0.000589 -0.001250   \n",
    "    # 2015-06-04 00:00:00+00:00  0.000651  0.002452  0.000482 -0.001096  0.003159   \n",
    "    # 2015-06-05 00:00:00+00:00  0.002385  0.000312 -0.000931  0.002899 -0.001469   \n",
    "\n",
    "    #                                 11D       12D       13D       14D  \n",
    "    # date                                                               \n",
    "    # 2015-06-01 00:00:00+00:00  0.000070 -0.001366  0.002941 -0.001755  \n",
    "    # 2015-06-02 00:00:00+00:00 -0.001240  0.003079 -0.001384 -0.001057  \n",
    "    # 2015-06-03 00:00:00+00:00  0.003016 -0.001379 -0.000775 -0.001095  \n",
    "    # 2015-06-04 00:00:00+00:00 -0.001151 -0.001089 -0.000949 -0.003318  \n",
    "    # 2015-06-05 00:00:00+00:00 -0.001145 -0.001061 -0.003247  0.001657      \n",
    "    \n",
    "    \n",
    "    portfolio_returns_total.columns = portfolio_returns_total.columns.map(lambda x: int(x[:-1]))\n",
    "\n",
    "    # Factorが掛けられたポートフォリオが生み出す毎日の損益を１日〜１５日分シフトする\n",
    "    # これは、その日Factorによって作られたポートフォリオが１日〜１５日後にどのくらいの\n",
    "    # 利益を生んでいるのか確認するため\n",
    "    for i in portfolio_returns_total.columns:\n",
    "        portfolio_returns_total[i] = portfolio_returns_total[i].shift(i)\n",
    "\n",
    "    # シフトしたデータでシャープレシオを算出\n",
    "    delay_sharpes_total = portfolio_returns_total.apply(ep.sharpe_ratio)\n",
    "    \n",
    "    factor = factor_data_total.factor\n",
    "    turnover = compute_turnover(factor)\n",
    "    n_holdings = factor.groupby(level=0).count()\n",
    "    perc_holdings = get_max_median_position_concentration(factor)\n",
    "    \n",
    "    return {'factor_data_total': factor_data_total, \n",
    "            'portfolio_returns_total': portfolio_returns_total,\n",
    "            'delay_sharpes_total': delay_sharpes_total,\n",
    "            'turnover': turnover,\n",
    "            'n_holdings': n_holdings,\n",
    "            'perc_holdings': perc_holdings,\n",
    "    }\n",
    "\n",
    "def plot_overview_tear_sheet(factor_data, periods=range(1, 15)):\n",
    "    # We assume portfolio weights, so make sure factor scores sum to 1\n",
    "    factor_data['factor'] = factor_data.factor.div(factor_data.abs().groupby(level='date').sum()['factor'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = plt.GridSpec(3, 4)\n",
    "    ax1 = plt.subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    factor_stats = compute_factor_stats(factor_data, periods=periods)\n",
    "    \n",
    "    # 描画１\n",
    "    # １日〜１５日後のシャープレシオを棒グラフで描画\n",
    "    pd.DataFrame({'total': factor_stats['delay_sharpes_total']}).plot.bar(ax=ax1)\n",
    "    ax1.set(xlabel='delay', ylabel='IR')\n",
    "\n",
    "    # 描画2\n",
    "    # １日目〜５日目のCumulative Returnを線グラフで描画\n",
    "    ax2a = plt.subplot(gs[0:2, 2:4])\n",
    "    delay_cum_rets_total = factor_stats['portfolio_returns_total'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_total.plot(ax=ax2a)\n",
    "    ax2a.set(title='Total returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    # 描画３の左軸。毎日の保有銘柄数\n",
    "    ax6 = plt.subplot(gs[-1, 0:2])\n",
    "    factor_stats['n_holdings'].plot(color='b', ax=ax6)\n",
    "    ax6.set_ylabel('# holdings', color='b')\n",
    "    ax6.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # 描画３の右軸。turnover。Factorの数値が毎日どのように変化するかを表現した線グラフ\n",
    "    # この変化が小さいほうがそのFactor がどんな市場状況でも安定した数字をだすので、信頼できるという意味になる（という事だと思う）\n",
    "    ax62 = ax6.twinx()\n",
    "    factor_stats['turnover'].plot(color='r', ax=ax62)\n",
    "    ax62.set_ylabel('turnover', color='r')\n",
    "    ax62.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    # Quantile毎の銘柄数。どのQuantileに何銘柄入っているかを描画\n",
    "    # 今回は売買simulation するわけではないですが、売買はこの一番高いQuantileに入っている銘柄を買い、一番低いQuantileの銘柄を\n",
    "    # 売るというポートフォリオを組みます\n",
    "    ax7 = plt.subplot(gs[-1, 2:4])\n",
    "    factor_stats['perc_holdings'].plot(ax=ax7)\n",
    "    ax7.set(ylabel='Long/short perc holdings')\n",
    "    \n",
    "    gs.tight_layout(fig)\n",
    "    \n",
    "    return fig, factor_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universe definition\n",
    "\n",
    "日本株用のユニバースを作っているところ。ここもさわらなくていい。\n",
    "\n",
    "ユニバースに入っている銘柄：\n",
    "\n",
    "+ 【過去2週間の最小出来高✕終値】のトップ30％（出来高が0以上の銘柄のみ）\n",
    "+ 最小出来高* 終値を算出することで、超低位株だから出来高が高い銘柄をユニバースから外している\n",
    "+ Japan（JP_EQUITIES）: Tokyo Stock Exchange, JASDAQ, Osaka Exchange, Nagoya Stock Exchange, Fukuoka Stock Exchange, Sapporo Securities Exchange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom factor that gets the minimum volume traded over the last two weeks.\n",
    "class MinVolume(CustomFactor):\n",
    "    inputs=[EquityPricing.volume]\n",
    "    window_length=10\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        # Calculates the column-wise standard deviation, ignoring NaNs\n",
    "        out[:] = np.min(values, axis=0)\n",
    "\n",
    "# Create a volume and price filter that filters for stocks in the top 30%.\n",
    "# We multiply by price to rule out penny stocks that trade in huge volume.\n",
    "volume_min = MinVolume()\n",
    "price = EquityPricing.close.latest\n",
    "univ_filter = ((price * volume_min).percentile_between(70, 100, mask=(volume_min > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your alpha factor here. Make sure to delete the following cell before making your submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our alpha factor is a size-based factor.\n",
    "alpha_factor = -factset.Fundamentals.mkt_val.latest.log1p()\n",
    "alpha_factor = factset.Fundamentals.roe_af.latest\n",
    "\n",
    "alpha_winsorized = alpha_factor.winsorize(min_percentile=0.05,\n",
    "                                          max_percentile=0.95,\n",
    "                                          mask=univ_filter)\n",
    "    \n",
    "# Zscore to get long and short (positive and negative) alphas to use as weights\n",
    "alpha_zscore = alpha_winsorized.zscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call evaluate_factor on our factor to get Alphalens-formatted data.\n",
    "al_data = evaluate_factor(\n",
    "    alpha_zscore, \n",
    "    JP_EQUITIES, \n",
    "    '2015-06-1', \n",
    "    '2016-06-1', \n",
    "    factor_screen=univ_filter,\n",
    "    returns_lengths=range(1, 15),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, factor_stats = plot_overview_tear_sheet(al_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats['factor_data_total'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats['delay_sharpes_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats['turnover'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats['portfolio_returns_total'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats['n_holdings'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats[\"perc_holdings\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = factor_stats['factor_data_total'].reset_index()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"asset\"][0].sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid(1178883450164305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x[\"asset\"] == x[\"asset\"][0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x[\"asset\"] == sid(1178883450164305)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
