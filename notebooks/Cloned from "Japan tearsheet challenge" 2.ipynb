{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan challenge submission template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.pipeline.data import EquityPricing, factset\n",
    "from quantopian.pipeline.factors import Returns, SimpleMovingAverage\n",
    "from quantopian.pipeline.data.factset.estimates import PeriodicConsensus\n",
    "from quantopian.pipeline.data.factset.estimates import Actuals\n",
    "from quantopian.pipeline.data.factset.estimates import LongTermConsensus\n",
    "\n",
    "from quantopian.pipeline.domain import (\n",
    "    AT_EQUITIES, # Austria\n",
    "    AU_EQUITIES, # Australia\n",
    "    BE_EQUITIES, # Belgium\n",
    "    BR_EQUITIES, # Brazil\n",
    "    CA_EQUITIES, # Canada\n",
    "    CH_EQUITIES, # Switzerland\n",
    "    CN_EQUITIES, # China\n",
    "    DE_EQUITIES, # Germany\n",
    "    DK_EQUITIES, # Denmark\n",
    "    ES_EQUITIES, # Spain\n",
    "    FI_EQUITIES, # Finland\n",
    "    FR_EQUITIES, # France\n",
    "    GB_EQUITIES, # Great Britain\n",
    "    HK_EQUITIES, # Hong Kong\n",
    "    IE_EQUITIES, # Ireland\n",
    "    IN_EQUITIES, # India\n",
    "    IT_EQUITIES, # Italy\n",
    "    JP_EQUITIES, # Japan\n",
    "    KR_EQUITIES, # South Korea\n",
    "    NL_EQUITIES, # Netherlands\n",
    "    NO_EQUITIES, # Norway\n",
    "    NZ_EQUITIES, # New Zealand\n",
    "    PT_EQUITIES, # Portugal\n",
    "    SE_EQUITIES, # Sweden\n",
    "    SG_EQUITIES, # Singapore\n",
    "    US_EQUITIES, # United States\n",
    ")\n",
    "from quantopian.research import run_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterExtend(CustomFactor):\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        out[:] = np.max(values, axis=0)\n",
    "\n",
    "def evaluate_factor(factor, \n",
    "                    domain, \n",
    "                    start_date, \n",
    "                    end_date,\n",
    "                    factor_screen=None,\n",
    "                    quantiles=5,\n",
    "                    returns_lengths=(1, 5, 10)):\n",
    "    \"\"\"Analyze a Pipeline Factor using Alphalens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor : quantopian.pipeline.factors.Factor\n",
    "        Factor producing scores to be evaluated.\n",
    "    domain : quantopian.pipeline.domain.Domain\n",
    "        Domain on which the factor should be evaluated.\n",
    "    start_date : str or pd.Timestamp\n",
    "        Start date for evaluation period.\n",
    "    end_date : str or pd.Timestamp\n",
    "        End date for evaluation period.\n",
    "    standardize : \n",
    "    factor_screen : quantopian.pipeline.filters.Filter, optional\n",
    "        Filter defining which assets ``factor`` should be evaluated on.\n",
    "        Default is ``factor.notnull()``.\n",
    "    quantiles : int, optional\n",
    "        Number of buckets to use for quantile groups. Default is 5\n",
    "    returns_lengths : sequence[int]\n",
    "        Forward-returns horizons to use when evaluating ``factor``. \n",
    "        Default is 1-day, 5-day, and 10-day returns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    factor_data : pd.DataFrame\n",
    "        A (date, asset)-indexed DataFrame with the following columns:\n",
    "            'factor': float64\n",
    "                Values produced by ``factor``.\n",
    "            'factor_quantiles': int64\n",
    "                Daily quantile label for each\n",
    "    \"\"\"\n",
    "    calendar = domain.calendar\n",
    "    # Roll input dates to the next trading session.\n",
    "    start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "    end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "    if factor_screen is None:\n",
    "        factor_screen = factor.notnull()\n",
    "        \n",
    "    # Run pipeline to get factor values and quantiles.\n",
    "    factor_pipe = Pipeline(\n",
    "        {'factor': factor, \n",
    "         'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "        screen=factor_screen,\n",
    "        domain=domain,\n",
    "    )\n",
    "    factor_results = run_pipeline(factor_pipe, start_date, end_date, chunksize=250)\n",
    "        \n",
    "    returns_pipe = Pipeline(\n",
    "        columns={'daily_returns': Returns(window_length=2)}, \n",
    "        domain=domain, \n",
    "        screen=FilterExtend(\n",
    "            inputs=[factor_screen], \n",
    "            window_length=max(returns_lengths)+1\n",
    "        ).eq(1)\n",
    "    )\n",
    "    \n",
    "    # Compute returns for the period after the factor pipeline, then \n",
    "    # shift the results back to align with our factor values.\n",
    "    returns_start_date = start_date\n",
    "    returns_end_date = end_date + domain.calendar.day * max(returns_lengths)\n",
    "    raw_returns = run_pipeline(returns_pipe, returns_start_date, returns_end_date, chunksize=500)\n",
    "    \n",
    "    shifted_returns = {}\n",
    "    column_order = []\n",
    "    daily_returns = raw_returns['daily_returns']\n",
    "    for length in returns_lengths:\n",
    "        # Shift 1-day returns back by a day, 5-day returns back by 5 days, etc.\n",
    "        colname = '{}D'.format(length)\n",
    "        column_order.append(colname)\n",
    "        shifted_returns[colname] = backshift_returns_series(daily_returns, length)\n",
    "        \n",
    "    # Merge backshifted returns into a single frame indexed like our desired output.\n",
    "    merged_returns = pd.DataFrame(\n",
    "        data=shifted_returns, \n",
    "        index=factor_results.index, \n",
    "        columns=column_order,\n",
    "    )\n",
    "    \n",
    "    # Concat factor results and forward returns column-wise.\n",
    "    merged = pd.concat([factor_results, merged_returns], axis=1)\n",
    "    merged.index.set_names(['date', 'asset'], inplace=True)\n",
    "    \n",
    "    # Drop NaNs\n",
    "    merged = merged.dropna(how='any')\n",
    "    \n",
    "    # Add a Business Day Offset to the DateTimeIndex\n",
    "    merged.index.levels[0].freq = pd.tseries.offsets.BDay()\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "    assert new_date_labels[0] == 0\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "    return pd.Series(data=new_values, index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "\n",
    "def compute_turnover(df):\n",
    "    return df.dropna().unstack().dropna(how='all').fillna(0).diff().abs().sum(1)\n",
    "\n",
    "def get_max_median_position_concentration(expos):\n",
    "    longs = expos.loc[expos > 0]\n",
    "    shorts = expos.loc[expos < 0]\n",
    "\n",
    "    return expos.groupby(level=0).quantile([.05, .25, .5, .75, .95]).unstack()\n",
    "\n",
    "def compute_factor_stats(factor_data_total, periods=range(1, 15)):\n",
    "    portfolio_returns_total = al.performance.factor_returns(factor_data_total)\n",
    "    portfolio_returns_total.columns = portfolio_returns_total.columns.map(lambda x: int(x[:-1]))\n",
    "    for i in portfolio_returns_total.columns:\n",
    "        portfolio_returns_total[i] = portfolio_returns_total[i].shift(i)\n",
    "\n",
    "    delay_sharpes_total = portfolio_returns_total.apply(ep.sharpe_ratio)\n",
    "    \n",
    "    factor = factor_data_total.factor\n",
    "    turnover = compute_turnover(factor)\n",
    "    n_holdings = factor.groupby(level=0).count()\n",
    "    perc_holdings = get_max_median_position_concentration(factor)\n",
    "    \n",
    "    return {'factor_data_total': factor_data_total, \n",
    "            'portfolio_returns_total': portfolio_returns_total,\n",
    "            'delay_sharpes_total': delay_sharpes_total,\n",
    "            'turnover': turnover,\n",
    "            'n_holdings': n_holdings,\n",
    "            'perc_holdings': perc_holdings,\n",
    "    }\n",
    "\n",
    "def plot_overview_tear_sheet(factor_data, periods=range(1, 15)):\n",
    "    # We assume portfolio weights, so make sure factor scores sum to 1\n",
    "    factor_data['factor'] = factor_data.factor.div(factor_data.abs().groupby(level='date').sum()['factor'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = plt.GridSpec(3, 4)\n",
    "    ax1 = plt.subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    factor_stats = compute_factor_stats(factor_data, periods=periods)\n",
    "                         \n",
    "    pd.DataFrame({'total': factor_stats['delay_sharpes_total']}).plot.bar(ax=ax1)\n",
    "    ax1.set(xlabel='delay', ylabel='IR')\n",
    "\n",
    "    ax2a = plt.subplot(gs[0:2, 2:4])\n",
    "    delay_cum_rets_total = factor_stats['portfolio_returns_total'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_total.plot(ax=ax2a)\n",
    "    ax2a.set(title='Total returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    ax6 = plt.subplot(gs[-1, 0:2])\n",
    "    factor_stats['n_holdings'].plot(color='b', ax=ax6)\n",
    "    ax6.set_ylabel('# holdings', color='b')\n",
    "    ax6.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax62 = ax6.twinx()\n",
    "    factor_stats['turnover'].plot(color='r', ax=ax62)\n",
    "    ax62.set_ylabel('turnover', color='r')\n",
    "    ax62.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    ax7 = plt.subplot(gs[-1, 2:4])\n",
    "    factor_stats['perc_holdings'].plot(ax=ax7)\n",
    "    ax7.set(ylabel='Long/short perc holdings')\n",
    "    \n",
    "    gs.tight_layout(fig)\n",
    "    \n",
    "    return fig, factor_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universe definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom factor that gets the minimum volume traded over the last two weeks.\n",
    "class MinVolume(CustomFactor):\n",
    "    inputs=[EquityPricing.volume]\n",
    "    window_length=10\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        # Calculates the column-wise standard deviation, ignoring NaNs\n",
    "        out[:] = np.min(values, axis=0)\n",
    "\n",
    "# Create a volume and price filter that filters for stocks in the top 30%.\n",
    "# We multiply by price to rule out penny stocks that trade in huge volume.\n",
    "volume_min = MinVolume()\n",
    "price = EquityPricing.close.latest\n",
    "univ_filter = ((price * volume_min).percentile_between(70, 100, mask=(volume_min > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your alpha factor here. Make sure to delete the following cell before making your submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call evaluate_factor on our factor to get Alphalens-formatted data.\n",
    "al_data = evaluate_factor(\n",
    "    alpha_zscore, \n",
    "    JP_EQUITIES, \n",
    "    '2015-06-1', \n",
    "    '2018-10-1', \n",
    "    factor_screen=univ_filter,\n",
    "    returns_lengths=range(1, 15),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview_tear_sheet(al_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
