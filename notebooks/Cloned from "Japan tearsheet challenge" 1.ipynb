{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan challenge submission template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantopian.pipeline import Pipeline, CustomFactor\n",
    "from quantopian.pipeline.data import EquityPricing, factset\n",
    "from quantopian.pipeline.factors import Returns, SimpleMovingAverage\n",
    "from quantopian.pipeline.data.factset.estimates import PeriodicConsensus\n",
    "from quantopian.pipeline.data.factset.estimates import Actuals\n",
    "\n",
    "from quantopian.pipeline.domain import (\n",
    "    AT_EQUITIES, # Austria\n",
    "    AU_EQUITIES, # Australia\n",
    "    BE_EQUITIES, # Belgium\n",
    "    BR_EQUITIES, # Brazil\n",
    "    CA_EQUITIES, # Canada\n",
    "    CH_EQUITIES, # Switzerland\n",
    "    CN_EQUITIES, # China\n",
    "    DE_EQUITIES, # Germany\n",
    "    DK_EQUITIES, # Denmark\n",
    "    ES_EQUITIES, # Spain\n",
    "    FI_EQUITIES, # Finland\n",
    "    FR_EQUITIES, # France\n",
    "    GB_EQUITIES, # Great Britain\n",
    "    HK_EQUITIES, # Hong Kong\n",
    "    IE_EQUITIES, # Ireland\n",
    "    IN_EQUITIES, # India\n",
    "    IT_EQUITIES, # Italy\n",
    "    JP_EQUITIES, # Japan\n",
    "    KR_EQUITIES, # South Korea\n",
    "    NL_EQUITIES, # Netherlands\n",
    "    NO_EQUITIES, # Norway\n",
    "    NZ_EQUITIES, # New Zealand\n",
    "    PT_EQUITIES, # Portugal\n",
    "    SE_EQUITIES, # Sweden\n",
    "    SG_EQUITIES, # Singapore\n",
    "    US_EQUITIES, # United States\n",
    ")\n",
    "from quantopian.research import run_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_factor(factor, \n",
    "                    domain, \n",
    "                    start_date, \n",
    "                    end_date,\n",
    "                    factor_screen=None,\n",
    "                    quantiles=5,\n",
    "                    returns_lengths=(1, 5, 10)):\n",
    "    \"\"\"Analyze a Pipeline Factor using Alphalens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor : quantopian.pipeline.factors.Factor\n",
    "        Factor producing scores to be evaluated.\n",
    "    domain : quantopian.pipeline.domain.Domain\n",
    "        Domain on which the factor should be evaluated.\n",
    "    start_date : str or pd.Timestamp\n",
    "        Start date for evaluation period.\n",
    "    end_date : str or pd.Timestamp\n",
    "        End date for evaluation period.\n",
    "    standardize : \n",
    "    factor_screen : quantopian.pipeline.filters.Filter, optional\n",
    "        Filter defining which assets ``factor`` should be evaluated on.\n",
    "        Default is ``factor.notnull()``.\n",
    "    quantiles : int, optional\n",
    "        Number of buckets to use for quantile groups. Default is 5\n",
    "    returns_lengths : sequence[int]\n",
    "        Forward-returns horizons to use when evaluating ``factor``. \n",
    "        Default is 1-day, 5-day, and 10-day returns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    factor_data : pd.DataFrame\n",
    "        A (date, asset)-indexed DataFrame with the following columns:\n",
    "            'factor': float64\n",
    "                Values produced by ``factor``.\n",
    "            'factor_quantiles': int64\n",
    "                Daily quantile label for each\n",
    "    \"\"\"\n",
    "    calendar = domain.calendar\n",
    "    # Roll input dates to the next trading session.\n",
    "    start_date = calendar.minute_to_session_label(pd.Timestamp(start_date, tz='UTC'))\n",
    "    end_date = calendar.minute_to_session_label(pd.Timestamp(end_date, tz='UTC'))\n",
    "    \n",
    "    if factor_screen is None:\n",
    "        factor_screen = factor.notnull()\n",
    "        \n",
    "    # Run pipeline to get factor values and quantiles.\n",
    "    factor_pipe = Pipeline(\n",
    "        {'factor': factor, \n",
    "         'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)},\n",
    "        screen=factor_screen,\n",
    "        domain=domain,\n",
    "    )\n",
    "    factor_results = run_pipeline(factor_pipe, start_date, end_date, chunksize=250)\n",
    "    \n",
    "    column_order = []\n",
    "    returns_cols = {}\n",
    "    for length in returns_lengths:\n",
    "        colname = '{}D'.format(length)\n",
    "        column_order.append(colname)\n",
    "        # Here we are not computing cumulative returns, this could be done\n",
    "        # more efficiently\n",
    "        returns_cols[colname] = Returns(window_length=2)\n",
    "    returns_pipe = Pipeline(returns_cols, domain=domain)\n",
    "    \n",
    "    # Compute returns for the period after the factor pipeline, then \n",
    "    # shift the results back to align with our factor values.\n",
    "    returns_start_date = start_date\n",
    "    returns_end_date = end_date + domain.calendar.day * max(returns_lengths)\n",
    "    raw_returns = run_pipeline(returns_pipe, returns_start_date, returns_end_date, chunksize=500)\n",
    "    \n",
    "    shifted_returns = {}\n",
    "    for name, length in zip(column_order, returns_lengths):\n",
    "        # Shift 1-day returns back by a day, 5-day returns back by 5 days, etc.\n",
    "        raw = raw_returns[name]\n",
    "        shifted_returns[name] = backshift_returns_series(raw, length)\n",
    "        \n",
    "    # Merge backshifted returns into a single frame indexed like our desired output.\n",
    "    merged_returns = pd.DataFrame(\n",
    "        data=shifted_returns, \n",
    "        index=factor_results.index, \n",
    "        columns=column_order,\n",
    "    )\n",
    "    \n",
    "    # Concat factor results and forward returns column-wise.\n",
    "    merged = pd.concat([factor_results, merged_returns], axis=1)\n",
    "    merged.index.set_names(['date', 'asset'], inplace=True)\n",
    "    \n",
    "    # Drop NaNs\n",
    "    merged = merged.dropna(how='any')\n",
    "    \n",
    "    # Add a Business Day Offset to the DateTimeIndex\n",
    "    merged.index.levels[0].freq = pd.tseries.offsets.BDay()\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def backshift_returns_series(series, N):\n",
    "    \"\"\"Shift a multi-indexed series backwards by N observations in the first level.\n",
    "    \n",
    "    This can be used to convert backward-looking returns into a forward-returns series.\n",
    "    \"\"\"\n",
    "    ix = series.index\n",
    "    dates, sids = ix.levels\n",
    "    date_labels, sid_labels = map(np.array, ix.labels)\n",
    "    # Output date labels will contain the all but the last N dates.\n",
    "    new_dates = dates[:-N]\n",
    "    # Output data will remove the first M rows, where M is the index of the\n",
    "    # last record with one of the first N dates.\n",
    "    cutoff = date_labels.searchsorted(N)\n",
    "    new_date_labels = date_labels[cutoff:] - N\n",
    "    new_sid_labels = sid_labels[cutoff:]\n",
    "    new_values = series.values[cutoff:]\n",
    "    assert new_date_labels[0] == 0\n",
    "    new_index = pd.MultiIndex(\n",
    "        levels=[new_dates, sids],\n",
    "        labels=[new_date_labels, new_sid_labels],\n",
    "        sortorder=1,\n",
    "        names=ix.names,\n",
    "    )\n",
    "    return pd.Series(data=new_values, index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import empyrical as ep\n",
    "import alphalens as al\n",
    "import pyfolio as pf\n",
    "\n",
    "def compute_turnover(df):\n",
    "    return df.dropna().unstack().dropna(how='all').fillna(0).diff().abs().sum(1)\n",
    "\n",
    "def get_max_median_position_concentration(expos):\n",
    "    longs = expos.loc[expos > 0]\n",
    "    shorts = expos.loc[expos < 0]\n",
    "\n",
    "    return expos.groupby(level=0).quantile([.05, .25, .5, .75, .95]).unstack()\n",
    "\n",
    "def compute_factor_stats(factor_data_total, periods=range(1, 15)):\n",
    "    portfolio_returns_total = al.performance.factor_returns(factor_data_total)\n",
    "    portfolio_returns_total.columns = portfolio_returns_total.columns.map(lambda x: int(x[:-1]))\n",
    "    for i in portfolio_returns_total.columns:\n",
    "        portfolio_returns_total[i] = portfolio_returns_total[i].shift(i)\n",
    "\n",
    "    delay_sharpes_total = portfolio_returns_total.apply(ep.sharpe_ratio)\n",
    "    \n",
    "    factor = factor_data_total.factor\n",
    "    turnover = compute_turnover(factor)\n",
    "    n_holdings = factor.groupby(level=0).count()\n",
    "    perc_holdings = get_max_median_position_concentration(factor)\n",
    "    \n",
    "    return {'factor_data_total': factor_data_total, \n",
    "            'portfolio_returns_total': portfolio_returns_total,\n",
    "            'delay_sharpes_total': delay_sharpes_total,\n",
    "            'turnover': turnover,\n",
    "            'n_holdings': n_holdings,\n",
    "            'perc_holdings': perc_holdings,\n",
    "    }\n",
    "\n",
    "def plot_overview_tear_sheet(factor_data, periods=range(1, 15)):\n",
    "    # We assume portfolio weights, so make sure factor scores sum to 1\n",
    "    factor_data['factor'] = factor_data.factor.div(factor_data.abs().groupby(level='date').sum()['factor'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    gs = plt.GridSpec(3, 4)\n",
    "    ax1 = plt.subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    factor_stats = compute_factor_stats(factor_data, periods=periods)\n",
    "                         \n",
    "    pd.DataFrame({'total': factor_stats['delay_sharpes_total']}).plot.bar(ax=ax1)\n",
    "    ax1.set(xlabel='delay', ylabel='IR')\n",
    "\n",
    "    ax2a = plt.subplot(gs[0:2, 2:4])\n",
    "    delay_cum_rets_total = factor_stats['portfolio_returns_total'][list(range(1, 5))].apply(ep.cum_returns)\n",
    "    delay_cum_rets_total.plot(ax=ax2a)\n",
    "    ax2a.set(title='Total returns', ylabel='Cumulative returns')\n",
    "    \n",
    "    ax6 = plt.subplot(gs[-1, 0:2])\n",
    "    factor_stats['n_holdings'].plot(color='b', ax=ax6)\n",
    "    ax6.set_ylabel('# holdings', color='b')\n",
    "    ax6.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax62 = ax6.twinx()\n",
    "    factor_stats['turnover'].plot(color='r', ax=ax62)\n",
    "    ax62.set_ylabel('turnover', color='r')\n",
    "    ax62.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    ax7 = plt.subplot(gs[-1, 2:4])\n",
    "    factor_stats['perc_holdings'].plot(ax=ax7)\n",
    "    ax7.set(ylabel='Long/short perc holdings')\n",
    "    \n",
    "    gs.tight_layout(fig)\n",
    "    \n",
    "    return fig, factor_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universe definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom factor that gets the minimum volume traded over the last two weeks.\n",
    "class MinVolume(CustomFactor):\n",
    "    inputs=[EquityPricing.volume]\n",
    "    window_length=10\n",
    "    def compute(self, today, asset_ids, out, values):\n",
    "        # Calculates the column-wise standard deviation, ignoring NaNs\n",
    "        out[:] = np.min(values, axis=0)\n",
    "\n",
    "# Create a volume and price filter that filters for stocks in the top 30%.\n",
    "# We multiply by price to rule out penny stocks that trade in huge volume.\n",
    "volume_min = MinVolume()\n",
    "price = EquityPricing.close.latest\n",
    "univ_filter = ((price * volume_min).percentile_between(70, 100, mask=(volume_min > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your alpha factor here. Make sure to delete the following cell before making your submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our alpha factor is a size-based factor.\n",
    "alpha_factor = -factset.Fundamentals.mkt_val.latest.log1p()\n",
    "alpha_winsorized = alpha_factor.winsorize(min_percentile=0.05,\n",
    "                                          max_percentile=0.95,\n",
    "                                          mask=univ_filter)\n",
    "    \n",
    "# Zscore to get long and short (positive and negative) alphas to use as weights\n",
    "alpha_zscore = alpha_winsorized.zscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_inc_cf_qf =  factset.Fundamentals.net_inc_cf_qf.latest\n",
    "mkt_val_public = factset.Fundamentals.mkt_val_public.latest\n",
    "per = mkt_val_public / net_inc_cf_qf\n",
    "\n",
    "fq1_eps_cons_mean = PeriodicConsensus.slice('EPS', 'qf', 1).mean.latest\n",
    "close = EquityPricing.close.latest\n",
    "per_predict = close / fq1_eps_cons_mean  \n",
    "\n",
    "# Acutual \n",
    "fq0_eps_act = Actuals.slice('EPS', 'qf', 0).actual_value.latest\n",
    "fq1_eps_act = Actuals.slice('EPS', 'qf', -1).actual_value.latest\n",
    "fq2_eps_act = Actuals.slice('EPS', 'qf', -2).actual_value.latest\n",
    "fq3_eps_act = Actuals.slice('EPS', 'qf', -3).actual_value.latest\n",
    "\n",
    "myfilter = (fq0_eps_act > fq1_eps_act) #& (fq1_eps_act > fq2_eps_act)\n",
    "\n",
    "\n",
    "alpha_factor =  fq0_eps_act / fq1_eps_act\n",
    "alpha_winsorized = alpha_factor.winsorize(min_percentile=0.05,\n",
    "                                          max_percentile=0.95,\n",
    "                                          mask=univ_filter #& myfilter\n",
    "                                         )\n",
    "\n",
    "alpha_zscore = alpha_winsorized.zscore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our alpha factor is a size-based factor.\n",
    "# from https://www.quantopian.com/posts/arithmetic-on-pipeline-factors\n",
    "# from https://www.quantopian.com/docs/data-reference/estimates_long_term\n",
    "import quantopian.pipeline.data.factset.estimates as fe\n",
    "\n",
    "# Slice the LongTermConsensus dataset family into datasets\n",
    "# for price target and long term EPS growth.\n",
    "\n",
    "# 6-12ヶ月の株価予測\n",
    "price_tgt_cons = fe.LongTermConsensus.slice('PRICE_TGT')\n",
    "# 3-5年のEPSの成長予測\n",
    "eps_gr_cons = fe.LongTermConsensus.slice('EPS_LTG')\n",
    "\n",
    "# Get the latest mean consensus price target and EPS growth.\n",
    "price_tgt_mean = price_tgt_cons.mean.latest\n",
    "price_tgt_low = price_tgt_cons.low.latest\n",
    "mean_low = (price_tgt_mean + price_tgt_low) / 2 \n",
    "eps_gr_mean = eps_gr_cons.mean.latest\n",
    "\n",
    "# Define an estimated price growth factor by taking the relative\n",
    "# difference between the 6-12 month price target and yesterday's\n",
    "# close price.\n",
    "# 終値\n",
    "yesterday_close = EquityPricing.close.latest\n",
    "\n",
    "fq1_eps_cons_mean = PeriodicConsensus.slice('EPS', 'qf', 1).mean.latest\n",
    "fq0_eps_act = Actuals.slice('EPS', 'qf', 0).actual_value.latest\n",
    "\n",
    "per_predict = yesterday_close / fq1_eps_cons_mean  \n",
    "\n",
    "\n",
    "# 株価予測と終値の比較()\n",
    "alpha_factor_est_price_growth = np.log1p((price_tgt_mean - yesterday_close) / yesterday_close)\n",
    "# 時価総額\n",
    "mkt_val = factset.Fundamentals.mkt_val.latest.log10() \n",
    "#\n",
    "alpha_factor_roe = factset.Fundamentals.roe_af.latest.log1p()\n",
    "\n",
    "# alpha_factor = 0.4 * alpha_factor_mkt_val + \\\n",
    "#                0.6 * (20 * alpha_factor_est_price_growth + alpha_factor_roe)\n",
    "#バックテストしながら適当に重みづけ Searching appropriate weight with Backtesting\n",
    "\n",
    "alpha_factor = price_tgt_mean / yesterday_close #* alpha_factor_roe\n",
    "alpha_factor = mean_low / yesterday_close\n",
    "\n",
    "\n",
    "alpha_winsorized = alpha_factor.winsorize(min_percentile=0.05,\n",
    "                                          max_percentile=0.95,\n",
    "                                          mask=univ_filter)\n",
    "\n",
    "alpha_zscore = alpha_winsorized.zscore()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call evaluate_factor on our factor to get Alphalens-formatted data.\n",
    "al_data = evaluate_factor(\n",
    "    alpha_zscore, #alpha_factor, #alpha_zscore, \n",
    "    JP_EQUITIES, \n",
    "    '2015-06-1', #'2015-06-1', \n",
    "    '2018-10-1',\n",
    "    factor_screen=univ_filter,\n",
    "    returns_lengths=range(1, 15),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, factor_stats = plot_overview_tear_sheet(al_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_stats[\"factor_data_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# PER \n",
    "net_inc_cf_qf =  factset.Fundamentals.net_inc_cf_qf.latest\n",
    "mkt_val_public = factset.Fundamentals.mkt_val_public.latest\n",
    "mini_mkt_val_public =  (10000000000 < mkt_val_public) & (mkt_val_public < 30000000000 )\n",
    "per = mkt_val_public / net_inc_cf_qf\n",
    "\n",
    "# 予想PER \n",
    "# 次の四半期のEPS予想と\n",
    "fq1_eps_cons_mean = PeriodicConsensus.slice('EPS', 'qf', 1).mean.latest\n",
    "close = EquityPricing.close.latest\n",
    "per_predict = close / fq1_eps_cons_mean   \n",
    "\n",
    "# Acutual \n",
    "fq0_eps_act = Actuals.slice('EPS', 'qf', 0).actual_value.latest\n",
    "fq1_eps_act = Actuals.slice('EPS', 'qf', -1).actual_value.latest\n",
    "fq2_eps_act = Actuals.slice('EPS', 'qf', -2).actual_value.latest\n",
    "fq3_eps_act = Actuals.slice('EPS', 'qf', -3).actual_value.latest\n",
    "\n",
    "# 経常利益\n",
    "ordinary_inc_qf =  factset.Fundamentals.ordinary_inc_qf.latest\n",
    "alpha_factor_mkt_val = factset.Fundamentals.mkt_val.latest.log10() \n",
    "\n",
    "\n",
    "factor_pipe = Pipeline(\n",
    "    {'net_inc_cf_qf': net_inc_cf_qf, \n",
    "     'mkt_val_public': mkt_val_public, \n",
    "     'alpha_factor_mkt_val':alpha_factor_mkt_val,\n",
    "     'per': per, \n",
    "     'fq1_eps_cons_mean':fq1_eps_cons_mean,\n",
    "     'close': close,\n",
    "     'per_predict':per_predict, \n",
    "     'fq0_eps_act':fq0_eps_act,\n",
    "     'fq1_eps_act':fq1_eps_act,\n",
    "     'fq2_eps_act':fq2_eps_act,\n",
    "     'fq3_eps_act':fq3_eps_act,\n",
    "     'ordinary_inc_qf':ordinary_inc_qf,\n",
    "     \n",
    "     \n",
    "     #'factor_quantile': factor.quantiles(quantiles, mask=factor_screen)\n",
    "    },\n",
    "    domain=JP_EQUITIES,\n",
    "    screen=mini_mkt_val_public,\n",
    ")\n",
    "factor_results = run_pipeline(factor_pipe, \"2018-1-1\", \"2018-10-1\", chunksize=250)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(factor_results[\"mkt_val_public\"].unstack().iloc[1] / 100000000).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(factor_results[\"alpha_factor_mkt_val\"].unstack().iloc[0] ).hist(bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
